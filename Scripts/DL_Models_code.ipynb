{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VA7T5KalyH2",
    "outputId": "006e844a-1021-4b7b-ddf5-8b7bab85a68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# Authors: Dinesh M, \n",
    "#          Ragothaman M. Yennamalli, SASTRA Deemed to be University\n",
    "# Last Modified: 2025-05-11\n",
    "# Description:\n",
    "#   This python scripts implements various deep learning models for PANN classification in Tamil poetry.\n",
    "#   It loads preprocessed embeddings and PANN labels from structured datasets.\n",
    "#   Multiple architectures like RNN, FFNN, CNN, LSTM, BiLSTM, MLP, GRU, and Transformer are trained and evaluated.\n",
    "#   Model performance is assessed using metrics like Precision, Recall, and F1-score.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kq32GiUvRFxG",
    "outputId": "6d489b62-a6d3-4d46-9e46-6a4a42cbc479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 281.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.7816\n",
      "\n",
      "ðŸ“š Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 461.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.4488\n",
      "\n",
      "ðŸ“š Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 458.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.2719\n",
      "\n",
      "ðŸ“š Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 462.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.1624\n",
      "\n",
      "ðŸ“š Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 398.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0799\n",
      "\n",
      "ðŸ“š Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 217.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0314\n",
      "\n",
      "ðŸ“š Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 469.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.9772\n",
      "\n",
      "ðŸ“š Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 461.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.9443\n",
      "\n",
      "ðŸ“š Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 452.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.8985\n",
      "\n",
      "ðŸ“š Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 457.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.8808\n",
      "\n",
      "ðŸ“š Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 454.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.8498\n",
      "\n",
      "ðŸ“š Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 460.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.8298\n",
      "\n",
      "ðŸ“š Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 451.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.8161\n",
      "\n",
      "ðŸ“š Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 456.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.8028\n",
      "\n",
      "ðŸ“š Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 461.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.7915\n",
      "\n",
      "ðŸ“š Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 354.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.7709\n",
      "\n",
      "ðŸ“š Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 357.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.7594\n",
      "\n",
      "ðŸ“š Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 417.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.7709\n",
      "\n",
      "ðŸ“š Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 448.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.7477\n",
      "\n",
      "ðŸ“š Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:00<00:00, 454.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 0.7519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 983.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       141\n",
      "           1       0.39      0.21      0.27       134\n",
      "           2       0.73      0.78      0.75       139\n",
      "           3       0.51      0.36      0.42       124\n",
      "           4       0.59      0.50      0.54       115\n",
      "           5       0.99      1.00      1.00       135\n",
      "           6       0.62      0.54      0.58       113\n",
      "           7       0.85      0.92      0.88       114\n",
      "           8       0.80      0.89      0.84       128\n",
      "           9       0.66      0.70      0.68       146\n",
      "          10       0.68      0.69      0.68       134\n",
      "          11       1.00      1.00      1.00       149\n",
      "          12       0.99      1.00      0.99       149\n",
      "          13       0.87      0.95      0.91       139\n",
      "          14       0.64      0.49      0.55       134\n",
      "          15       0.64      0.56      0.60       151\n",
      "          16       0.62      0.62      0.62       124\n",
      "          17       0.99      1.00      0.99       136\n",
      "          18       0.91      0.97      0.94       118\n",
      "          19       0.57      0.55      0.56       121\n",
      "          20       0.90      0.94      0.92       140\n",
      "          21       0.97      1.00      0.99       137\n",
      "          22       0.71      0.76      0.73       120\n",
      "          23       0.99      1.00      1.00       141\n",
      "          24       0.63      0.77      0.69       134\n",
      "          25       0.69      0.67      0.68       169\n",
      "          26       0.92      1.00      0.96       133\n",
      "          27       0.82      0.84      0.83       147\n",
      "          28       0.85      0.89      0.87       143\n",
      "          29       0.96      1.00      0.98       130\n",
      "          30       0.89      1.00      0.94       137\n",
      "          31       0.94      0.99      0.97       150\n",
      "          32       0.99      1.00      0.99       151\n",
      "          33       0.99      1.00      0.99       156\n",
      "          34       0.78      0.82      0.80       145\n",
      "\n",
      "    accuracy                           0.82      4777\n",
      "   macro avg       0.80      0.81      0.80      4777\n",
      "weighted avg       0.81      0.82      0.81      4777\n",
      "\n",
      "ðŸ§© Confusion Matrix:\n",
      "[[141   0   0 ...   0   0   0]\n",
      " [  1  28   5 ...   1   0   4]\n",
      " [  0   3 109 ...   0   0   4]\n",
      " ...\n",
      " [  0   0   0 ... 151   0   0]\n",
      " [  0   0   0 ...   0 156   0]\n",
      " [  1   0   1 ...   0   0 119]]\n",
      "âœ… Accuracy:  0.8181\n",
      "âœ… Precision: 0.8064\n",
      "âœ… Recall:    0.8181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… F1 Score:  0.8100\n",
      "ðŸ’¾ Model saved as rnn_model.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers,\n",
    "                          batch_first=True, nonlinearity='tanh')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        out = rnn_out[:, -1, :]  \n",
    "        out = self.dropout(out)\n",
    "        return self.classifier(out)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y = df['PANN_LABEL'].values\n",
    "    return X, y\n",
    "\n",
    "def run_rnn(train_file, val_file, test_file):\n",
    "    X_train, y_train = load_data(train_file)\n",
    "    X_val, y_val = load_data(val_file)\n",
    "    X_test, y_test = load_data(test_file)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_val_enc = label_encoder.transform(y_val)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_loader = DataLoader(EmbeddingDataset(X_train, y_train_enc), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(EmbeddingDataset(X_val, y_val_enc), batch_size=32)\n",
    "    test_loader = DataLoader(EmbeddingDataset(X_test, y_test_enc), batch_size=32)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = RNNClassifier(input_dim=input_dim, hidden_dim=128, num_classes=num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        print(f\"\\n Epoch {epoch+1}\")\n",
    "        train_model(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\" Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\" Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    y_true, y_pred = evaluate_model(model, test_loader)\n",
    "    print(\"\\n Classification Report (Test):\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\" Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\" Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" Recall:    {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" F1 Score:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"rnn_model.pt\")\n",
    "    print(\"ðŸ’¾ Model saved as rnn_model.pt\")\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "run_rnn(train_file, val_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bL5iZJcJmOVL",
    "outputId": "45f604c6-c447-4b15-f722-04f690b3a3a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 209.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.7274\n",
      "\n",
      "ðŸ“š Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 420.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.6063\n",
      "\n",
      "ðŸ“š Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 331.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.5377\n",
      "\n",
      "ðŸ“š Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 314.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.5216\n",
      "\n",
      "ðŸ“š Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 425.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.5139\n",
      "\n",
      "ðŸ“š Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 418.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4875\n",
      "\n",
      "ðŸ“š Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 411.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4657\n",
      "\n",
      "ðŸ“š Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 404.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4615\n",
      "\n",
      "ðŸ“š Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 426.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4478\n",
      "\n",
      "ðŸ“š Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 419.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4420\n",
      "\n",
      "ðŸ“š Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 419.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4444\n",
      "\n",
      "ðŸ“š Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 381.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4316\n",
      "\n",
      "ðŸ“š Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 346.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4312\n",
      "\n",
      "ðŸ“š Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 355.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4205\n",
      "\n",
      "ðŸ“š Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 412.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4085\n",
      "\n",
      "ðŸ“š Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 428.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4056\n",
      "\n",
      "ðŸ“š Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 403.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4031\n",
      "\n",
      "ðŸ“š Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 413.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.4047\n",
      "\n",
      "ðŸ“š Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 425.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.3904\n",
      "\n",
      "ðŸ“š Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:01<00:00, 418.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 3.3839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 612.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.13      0.15       141\n",
      "           1       0.08      0.43      0.13       134\n",
      "           2       0.27      0.05      0.08       139\n",
      "           3       0.07      0.12      0.09       124\n",
      "           4       0.00      0.00      0.00       115\n",
      "           5       0.25      0.81      0.38       135\n",
      "           6       0.00      0.00      0.00       113\n",
      "           7       0.10      0.04      0.05       114\n",
      "           8       0.12      0.04      0.06       128\n",
      "           9       0.00      0.00      0.00       146\n",
      "          10       0.03      0.02      0.02       134\n",
      "          11       0.19      0.48      0.27       149\n",
      "          12       0.15      0.16      0.16       149\n",
      "          13       0.05      0.01      0.02       139\n",
      "          14       0.00      0.00      0.00       134\n",
      "          15       0.06      0.01      0.01       151\n",
      "          16       0.00      0.00      0.00       124\n",
      "          17       0.10      0.04      0.05       136\n",
      "          18       0.00      0.00      0.00       118\n",
      "          19       0.00      0.00      0.00       121\n",
      "          20       0.00      0.00      0.00       140\n",
      "          21       0.11      0.20      0.14       137\n",
      "          22       0.00      0.00      0.00       120\n",
      "          23       0.16      0.29      0.21       141\n",
      "          24       0.09      0.04      0.05       134\n",
      "          25       0.00      0.00      0.00       169\n",
      "          26       0.09      0.17      0.12       133\n",
      "          27       0.00      0.00      0.00       147\n",
      "          28       0.04      0.03      0.04       143\n",
      "          29       0.16      0.58      0.26       130\n",
      "          30       0.07      0.10      0.08       137\n",
      "          31       0.05      0.01      0.02       150\n",
      "          32       0.14      0.66      0.23       151\n",
      "          33       0.00      0.00      0.00       156\n",
      "          34       0.00      0.00      0.00       145\n",
      "\n",
      "    accuracy                           0.13      4777\n",
      "   macro avg       0.07      0.13      0.08      4777\n",
      "weighted avg       0.07      0.13      0.08      4777\n",
      "\n",
      "ðŸ§© Confusion Matrix:\n",
      "[[ 19  20   0 ...  28   0   0]\n",
      " [  0  58   1 ...   5   0   0]\n",
      " [  1  18   7 ...  21   0   0]\n",
      " ...\n",
      " [  1   7   0 ... 100   0   0]\n",
      " [  5   4   1 ...  32   0   0]\n",
      " [  3  27   1 ...  15   1   0]]\n",
      "âœ… Accuracy:  0.1290\n",
      "âœ… Precision: 0.0734\n",
      "âœ… Recall:    0.1290\n",
      "âœ… F1 Score:  0.0763\n",
      "ðŸ’¾ Model saved as cnn_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_filters=100, kernel_sizes=[2, 3, 4], dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) \n",
    "        x = [torch.relu(conv(x)).squeeze(2) for conv in self.convs]\n",
    "        x = [torch.max(feature_map, dim=2)[0] if feature_map.dim() == 3 else feature_map for feature_map in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y = df['PANN_LABEL'].values\n",
    "    return X, y\n",
    "\n",
    "def run_cnn(train_file, val_file, test_file):\n",
    "    X_train, y_train = load_data(train_file)\n",
    "    X_val, y_val = load_data(val_file)\n",
    "    X_test, y_test = load_data(test_file)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_val_enc = label_encoder.transform(y_val)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_loader = DataLoader(EmbeddingDataset(X_train, y_train_enc), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(EmbeddingDataset(X_val, y_val_enc), batch_size=32)\n",
    "    test_loader = DataLoader(EmbeddingDataset(X_test, y_test_enc), batch_size=32)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = CNNClassifier(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        print(f\"\\n Epoch {epoch+1}\")\n",
    "        train_model(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\" Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\" Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    y_true, y_pred = evaluate_model(model, test_loader)\n",
    "    print(\"\\n Classification Report (Test):\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\" Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\" Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" Recall:    {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" F1 Score:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"cnn_model.pt\")\n",
    "    print(\"ðŸ’¾ Model saved as cnn_model.pt\")\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "run_cnn(train_file, val_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80HUCWgFoc6t",
    "outputId": "eefce463-57f3-4fc5-f706-d1f6d82a11d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 149.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.9267\n",
      "\n",
      "ðŸ“š Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:03<00:00, 134.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.7222\n",
      "\n",
      "ðŸ“š Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 163.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.5805\n",
      "\n",
      "ðŸ“š Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 162.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.4280\n",
      "\n",
      "ðŸ“š Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 163.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.4133\n",
      "\n",
      "ðŸ“š Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:03<00:00, 136.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.3234\n",
      "\n",
      "ðŸ“š Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 163.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.2939\n",
      "\n",
      "ðŸ“š Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 163.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.2408\n",
      "\n",
      "ðŸ“š Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 163.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.2226\n",
      "\n",
      "ðŸ“š Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:03<00:00, 142.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.1715\n",
      "\n",
      "ðŸ“š Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 155.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.1297\n",
      "\n",
      "ðŸ“š Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 161.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.1474\n",
      "\n",
      "ðŸ“š Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 161.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.1175\n",
      "\n",
      "ðŸ“š Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:03<00:00, 145.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0829\n",
      "\n",
      "ðŸ“š Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 150.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0561\n",
      "\n",
      "ðŸ“š Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 161.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0646\n",
      "\n",
      "ðŸ“š Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 161.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0331\n",
      "\n",
      "ðŸ“š Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:03<00:00, 148.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0058\n",
      "\n",
      "ðŸ“š Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:03<00:00, 146.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0177\n",
      "\n",
      "ðŸ“š Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448/448 [00:02<00:00, 160.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Validation Loss: 1.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 818.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       141\n",
      "           1       0.21      0.21      0.21       134\n",
      "           2       0.63      0.56      0.59       139\n",
      "           3       0.22      0.15      0.18       124\n",
      "           4       0.41      0.26      0.32       115\n",
      "           5       0.99      1.00      1.00       135\n",
      "           6       0.28      0.36      0.32       113\n",
      "           7       0.76      0.84      0.80       114\n",
      "           8       0.78      0.77      0.78       128\n",
      "           9       0.67      0.68      0.68       146\n",
      "          10       0.57      0.41      0.48       134\n",
      "          11       0.98      1.00      0.99       149\n",
      "          12       0.94      1.00      0.97       149\n",
      "          13       0.73      0.96      0.83       139\n",
      "          14       0.66      0.29      0.40       134\n",
      "          15       0.51      0.57      0.54       151\n",
      "          16       0.62      0.49      0.55       124\n",
      "          17       0.94      1.00      0.97       136\n",
      "          18       0.89      0.93      0.91       118\n",
      "          19       0.30      0.33      0.32       121\n",
      "          20       0.91      0.78      0.84       140\n",
      "          21       0.91      1.00      0.95       137\n",
      "          22       0.44      0.62      0.51       120\n",
      "          23       0.97      1.00      0.99       141\n",
      "          24       0.66      0.49      0.56       134\n",
      "          25       0.69      0.44      0.54       169\n",
      "          26       0.95      1.00      0.97       133\n",
      "          27       0.58      0.81      0.67       147\n",
      "          28       0.76      0.75      0.75       143\n",
      "          29       0.96      1.00      0.98       130\n",
      "          30       0.82      0.99      0.89       137\n",
      "          31       0.86      0.87      0.86       150\n",
      "          32       0.98      1.00      0.99       151\n",
      "          33       0.96      1.00      0.98       156\n",
      "          34       0.59      0.68      0.63       145\n",
      "\n",
      "    accuracy                           0.73      4777\n",
      "   macro avg       0.72      0.72      0.71      4777\n",
      "weighted avg       0.73      0.73      0.72      4777\n",
      "\n",
      "ðŸ§© Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140   0   0 ...   0   0   0]\n",
      " [  2  28   2 ...   1   1   7]\n",
      " [  0   2  78 ...   0   0  11]\n",
      " ...\n",
      " [  0   0   0 ... 151   0   0]\n",
      " [  0   0   0 ...   0 156   0]\n",
      " [  1   1   0 ...   0   0  98]]\n",
      "âœ… Accuracy:  0.7293\n",
      "âœ… Precision: 0.7254\n",
      "âœ… Recall:    0.7293\n",
      "âœ… F1 Score:  0.7206\n",
      "ðŸ’¾ Model saved as transformer_model.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, nhead=4, num_layers=2, dim_feedforward=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1) \n",
    "        return self.classifier(x)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y = df['PANN_LABEL'].values\n",
    "    return X, y\n",
    "\n",
    "def run_transformer(train_file, val_file, test_file):\n",
    "    X_train, y_train = load_data(train_file)\n",
    "    X_val, y_val = load_data(val_file)\n",
    "    X_test, y_test = load_data(test_file)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_val_enc = label_encoder.transform(y_val)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_loader = DataLoader(EmbeddingDataset(X_train, y_train_enc), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(EmbeddingDataset(X_val, y_val_enc), batch_size=32)\n",
    "    test_loader = DataLoader(EmbeddingDataset(X_test, y_test_enc), batch_size=32)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = TransformerClassifier(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        print(f\"\\n Epoch {epoch+1}\")\n",
    "        train_model(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\" Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\" Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    y_true, y_pred = evaluate_model(model, test_loader)\n",
    "    print(\"\\n Classification Report (Test):\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\" Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\" Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" Recall:    {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" F1 Score:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "    print(\" Model saved as transformer_model.pt\")\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "run_transformer(train_file, val_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) \n",
    "        gru_out, _ = self.gru(x)\n",
    "        out = gru_out[:, -1, :]  \n",
    "        return self.classifier(out)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y = df['PANN_LABEL'].values\n",
    "    return X, y\n",
    "\n",
    "def run_gru(train_file, val_file, test_file):\n",
    "   \n",
    "    X_train, y_train = load_data(train_file)\n",
    "    X_val, y_val = load_data(val_file)\n",
    "    X_test, y_test = load_data(test_file)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_val_enc = label_encoder.transform(y_val)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_loader = DataLoader(EmbeddingDataset(X_train, y_train_enc), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(EmbeddingDataset(X_val, y_val_enc), batch_size=32)\n",
    "    test_loader = DataLoader(EmbeddingDataset(X_test, y_test_enc), batch_size=32)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = GRUClassifier(input_dim=input_dim, hidden_dim=128, num_classes=num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        print(f\"\\nðŸ“š Epoch {epoch+1}\")\n",
    "        train_model(model, train_loader, optimizer, criterion)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\" Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "  \n",
    "    y_true, y_pred = evaluate_model(model, test_loader)\n",
    "    print(\"\\n Classification Report (Test):\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\" Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\" Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" Recall:    {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" F1 Score:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "\n",
    "   \n",
    "    torch.save(model.state_dict(), \"gru_model.pt\")\n",
    "    print(\"Model saved as gru_model.pt\")\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "run_gru(train_file, val_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class FFNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y = df['PANN_LABEL'].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_ffnn(train_file, val_file, test_file):\n",
    "    X_train, y_train = load_data(train_file)\n",
    "    X_val, y_val = load_data(val_file)\n",
    "    X_test, y_test = load_data(test_file)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_val_enc = label_encoder.transform(y_val)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_loader = DataLoader(EmbeddingDataset(X_train, y_train_enc), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(EmbeddingDataset(X_val, y_val_enc), batch_size=32)\n",
    "    test_loader = DataLoader(EmbeddingDataset(X_test, y_test_enc), batch_size=32)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = FFNNClassifier(input_dim=input_dim, hidden_dims=[256, 128], num_classes=num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopper = EarlyStopping(patience=3)\n",
    "    for epoch in range(20):\n",
    "        print(f\"\\n Epoch {epoch+1}\")\n",
    "        train_model(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\" Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    y_true, y_pred = evaluate_model(model, test_loader)\n",
    "    print(\"\\n Classification Report (Test):\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\" Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\" Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" Recall:    {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" F1 Score:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"ffnn_model.pt\")\n",
    "    print(\" Model saved as ffnn_model.pt\")\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "run_ffnn(train_file, val_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X_data = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y_data = df['PANN_LABEL'].values\n",
    "    return torch.tensor(X_data), torch.tensor(y_data)\n",
    "\n",
    "X_train, y_train = load_data(train_file)\n",
    "X_val, y_val = load_data(val_file)\n",
    "X_test, y_test = load_data(test_file)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, num_layers=2, output_size=32, dropout_rate=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]  # Take last time-step output\n",
    "        out = self.layernorm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(torch.unique(y_train))\n",
    "model = LSTMModel(input_size=input_size, output_size=output_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_model_state = None\n",
    "best_val_preds = None\n",
    "best_val_labels = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    y_val_true, y_val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_val_true.extend(labels.cpu().numpy())\n",
    "            y_val_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    val_accuracy = np.mean(np.array(y_val_true) == np.array(y_val_pred))\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        best_val_preds = y_val_pred.copy()\n",
    "        best_val_labels = y_val_true.copy()\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "print(\"\\n Best Validation Classification Report:\")\n",
    "print(classification_report(best_val_labels, best_val_preds, zero_division=0))\n",
    "\n",
    "model.eval()\n",
    "y_test_true, y_test_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_test_true.extend(labels.cpu().numpy())\n",
    "        y_test_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\n Test Classification Report:\")\n",
    "print(classification_report(y_test_true, y_test_pred, zero_division=0))\n",
    "\n",
    "print(\"\\n Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_true, y_test_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "precision = precision_score(y_test_true, y_test_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test_true, y_test_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test_true, y_test_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n Overall Test Accuracy:  {accuracy:.4f}\")\n",
    "print(f\" Overall Precision:      {precision:.4f}\")\n",
    "print(f\" Overall Recall:         {recall:.4f}\")\n",
    "print(f\" Overall F1-Score:       {f1:.4f}\")\n",
    "\n",
    "torch.save(best_model_state, \"best_lstm_model.pth\")\n",
    "print(\"\\n Best LSTM model saved as 'best_lstm_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch_size, seq_len=1, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]  # last timestep\n",
    "        return self.classifier(out)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y = df['PANN_LABEL'].values\n",
    "    return X, y\n",
    "\n",
    "def run_bilstm(train_file, val_file, test_file):\n",
    "    X_train, y_train = load_data(train_file)\n",
    "    X_val, y_val = load_data(val_file)\n",
    "    X_test, y_test = load_data(test_file)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_enc = label_encoder.fit_transform(y_train)\n",
    "    y_val_enc = label_encoder.transform(y_val)\n",
    "    y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_loader = DataLoader(EmbeddingDataset(X_train, y_train_enc), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(EmbeddingDataset(X_val, y_val_enc), batch_size=32)\n",
    "    test_loader = DataLoader(EmbeddingDataset(X_test, y_test_enc), batch_size=32)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = BiLSTMClassifier(input_dim, hidden_dim=128, num_classes=num_classes).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        print(f\"\\n Epoch {epoch+1}\")\n",
    "        train_model(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\" Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    y_true, y_pred = evaluate_model(model, test_loader)\n",
    "    print(\"\\nClassification Report (Test):\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\" Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\" Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\" Precision: {precision_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" Recall:    {recall_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\" F1 Score:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"bilstm_model.pt\")\n",
    "    print(\" Model saved as bilstm_model.pt\")\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "run_bilstm(train_file, val_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train_file = \"path_to_your_directory\"\n",
    "val_file = \"path_to_your_directory\"\n",
    "test_file = \"path_to_your_directory\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Embedding'] = df['Embedding'].apply(ast.literal_eval)\n",
    "    X_data = np.array(df['Embedding'].tolist(), dtype=np.float32)\n",
    "    y_data = df['PANN_LABEL'].values\n",
    "    return X_data, y_data\n",
    "\n",
    "X_train, y_train = load_data(train_file)\n",
    "X_val, y_val = load_data(val_file)\n",
    "X_test, y_test = load_data(test_file)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_val_enc = label_encoder.transform(y_val)\n",
    "y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_val_norm = scaler.transform(X_val)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "  \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_norm, y_train_enc)\n",
    "\n",
    "    print(\"ðŸ” Validation Results:\")\n",
    "    val_preds = model.predict(X_val_norm)\n",
    "    print(classification_report(y_val_enc, val_preds, zero_division=0))\n",
    "\n",
    "    print(\" Test Results:\")\n",
    "    test_preds = model.predict(X_test_norm)\n",
    "    print(classification_report(y_test_enc, test_preds, zero_division=0))\n",
    "\n",
    "    acc = accuracy_score(y_test_enc, test_preds)\n",
    "    prec = precision_score(y_test_enc, test_preds, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test_enc, test_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_enc, test_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\" Overall Test Accuracy:  {acc:.4f}\")\n",
    "    print(f\" Overall Precision:      {prec:.4f}\")\n",
    "    print(f\" Overall Recall:         {rec:.4f}\")\n",
    "    print(f\" Overall F1-Score:       {f1:.4f}\")\n",
    "\n",
    "    print(\" Confusion Matrix (Test):\")\n",
    "    print(confusion_matrix(y_test_enc, test_preds))\n",
    "\n",
    "    model_filename = f\"{name.replace(' ', '_').lower()}_model.pkl\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\" Saved model to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aAyQFHGm-BT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
